// Substrate-lite
// Copyright (C) 2019-2020  Parity Technologies (UK) Ltd.
// SPDX-License-Identifier: GPL-3.0-or-later WITH Classpath-exception-2.0

// This program is free software: you can redistribute it and/or modify
// it under the terms of the GNU General Public License as published by
// the Free Software Foundation, either version 3 of the License, or
// (at your option) any later version.

// This program is distributed in the hope that it will be useful,
// but WITHOUT ANY WARRANTY; without even the implied warranty of
// MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the
// GNU General Public License for more details.

// You should have received a copy of the GNU General Public License
// along with this program.  If not, see <http://www.gnu.org/licenses/>.

//! Background synchronization service.
//!
//! The [`SyncService`] manages a background task dedicated to synchronizing the chain with the
//! network.
//! Importantly, its design is oriented towards the particular use case of the full node.

// TODO: doc
// TODO: re-review this once finished

use core::{num::NonZeroU32, pin::Pin};
use futures::{
    channel::{mpsc, oneshot},
    lock::Mutex,
    prelude::*,
};
use std::{collections::BTreeMap, sync::Arc, time::SystemTime};
use substrate_lite::{
    chain::{self, sync::full_optimistic},
    network,
};

/// Configuration for a [`SyncService`].
pub struct Config<'a> {
    /// Closure that spawns background tasks.
    pub tasks_executor: Box<dyn FnMut(Pin<Box<dyn Future<Output = ()> + Send>>) + Send>,

    /// Information about the starting point of the chain.
    pub chain_information: chain::chain_information::ChainInformation,

    // TODO: remove
    pub chain_spec: &'a crate::chain_spec::ChainSpec,
}

/// Event generated by [`SyncService::next_event`].
#[derive(Debug)]
pub enum Event {
    BlocksRequest {
        id: BlocksRequestId,
        target: network::PeerId,
        request: network::protocol::BlocksRequestConfig,
    },
}

/// Identifier for a blocks request to be performed.
#[derive(Debug, Copy, Clone, Ord, PartialOrd, Eq, PartialEq, Hash)]
pub struct BlocksRequestId(usize);

/// Summary of the state of the [`SyncService`].
#[derive(Debug, Clone)]
pub struct SyncState {
    pub best_block_number: u64,
    pub best_block_hash: [u8; 32],
    pub finalized_block_number: u64,
    pub finalized_block_hash: [u8; 32],
}

/// Background task that verifies blocks and emits requests.
pub struct SyncService {
    /// State kept up-to-date with the background task.
    sync_state: Arc<Mutex<SyncState>>,

    /// Sender of messages towards the background task.
    to_background: Mutex<mpsc::Sender<ToBackground>>,

    /// Receiver of events sent by the background task.
    from_background: Mutex<mpsc::Receiver<FromBackground>>,

    /// For each emitted blocks request, an element is stored here.
    blocks_requests:
        Mutex<slab::Slab<oneshot::Sender<Result<Vec<network::protocol::BlockData>, ()>>>>,
}

impl SyncService {
    /// Initializes the [`SyncService`] with the given configuration.
    pub async fn new(mut config: Config<'_>) -> Arc<Self> {
        let (to_foreground, from_background) = mpsc::channel(16);
        let (to_background, from_foreground) = mpsc::channel(16);

        let sync_state = Arc::new(Mutex::new(SyncState {
            best_block_hash: [0; 32],      // TODO:
            best_block_number: 0,          // TODO:
            finalized_block_hash: [0; 32], // TODO:
            finalized_block_number: 0,     // TODO:
        }));

        (config.tasks_executor)(Box::pin(
            start_sync(
                config.chain_spec,
                config.chain_information,
                sync_state.clone(),
                to_foreground,
                from_foreground,
            )
            .await,
        ));

        Arc::new(SyncService {
            sync_state,
            to_background: Mutex::new(to_background),
            from_background: Mutex::new(from_background),
            blocks_requests: Mutex::new(slab::Slab::new()),
        })
    }

    /// Returns a summary of the state of the service.
    pub async fn sync_state(&self) -> SyncState {
        self.sync_state.lock().await.clone()
    }

    /// Registers a new source for blocks.
    pub async fn add_source(&self, peer_id: network::PeerId) {
        self.to_background
            .lock()
            .await
            .send(ToBackground::PeerConnected(peer_id))
            .await
            .unwrap()
    }

    /// Removes a source of blocks.
    pub async fn remove_source(&self, peer_id: network::PeerId) {
        self.to_background
            .lock()
            .await
            .send(ToBackground::PeerDisconnected(peer_id))
            .await
            .unwrap()
    }

    /// Sets the answer to a previously-emitted [`Event::BlocksRequest`].
    ///
    /// After this has been called, the `id` is no longer valid.
    ///
    /// # Panic
    ///
    /// Panics if the `id` is invalid.
    ///
    pub async fn answer_blocks_request(
        &self,
        id: BlocksRequestId,
        response: Result<Vec<network::protocol::BlockData>, ()>,
    ) {
        let _ = self
            .blocks_requests
            .lock()
            .await
            .remove(id.0)
            .send(response);
    }

    /// Returns the next event that happened in the sync service.
    ///
    /// If this method is called multiple times simultaneously, the events will be distributed
    /// amongst the different calls in an unpredictable way.
    pub async fn next_event(&self) -> Event {
        loop {
            match self.from_background.lock().await.next().await.unwrap() {
                FromBackground::RequestStart {
                    target,
                    request,
                    send_back,
                } => {
                    let id = BlocksRequestId(self.blocks_requests.lock().await.insert(send_back));
                    return Event::BlocksRequest {
                        id,
                        target,
                        request,
                    };
                }
            }
        }
    }
}

/// Message sent to the background task.
enum ToBackground {
    PeerConnected(network::PeerId),
    PeerDisconnected(network::PeerId),
}

/// Messsage sent from the background task and dedicated to the main [`SyncService`]. Processed
/// in [`SyncService::next_event`].
enum FromBackground {
    /// A blocks request must be started.
    RequestStart {
        target: network::PeerId,
        request: network::protocol::BlocksRequestConfig,
        send_back: oneshot::Sender<Result<Vec<network::protocol::BlockData>, ()>>, // TODO: proper error
    },
}

/// Returns the background task of the sync service.
async fn start_sync(
    chain_spec: &crate::chain_spec::ChainSpec,
    chain_information: chain::chain_information::ChainInformation,
    sync_state: Arc<Mutex<SyncState>>,
    mut to_foreground: mpsc::Sender<FromBackground>,
    mut from_foreground: mpsc::Receiver<ToBackground>,
) -> impl Future<Output = ()> {
    let mut sync =
        full_optimistic::OptimisticFullSync::<_, network::PeerId>::new(full_optimistic::Config {
            chain_information,
            sources_capacity: 32,
            blocks_capacity: {
                // This is the maximum number of blocks between two consecutive justifications.
                1024
            },
            source_selection_randomness_seed: rand::random(),
            blocks_request_granularity: NonZeroU32::new(128).unwrap(),
            download_ahead_blocks: {
                // Assuming a verification speed of 1k blocks/sec and a 95% latency of one second,
                // the number of blocks to download ahead of time in order to not block is 1000.
                1024
            },
        });

    let mut finalized_block_storage = BTreeMap::<Vec<u8>, Vec<u8>>::new();
    // TODO: doesn't necessarily match chain_information; pass this as part of the params of `start_sync` instead
    for (key, value) in chain_spec.genesis_storage() {
        finalized_block_storage.insert(key.to_owned(), value.to_owned());
    }

    async move {
        let mut peers_source_id_map = hashbrown::HashMap::<_, _, fnv::FnvBuildHasher>::default();
        let mut block_requests_finished = stream::FuturesUnordered::new();

        loop {
            let unix_time = SystemTime::now()
                .duration_since(SystemTime::UNIX_EPOCH)
                .unwrap();

            // Verify blocks that have been fetched from queries.
            let mut process = sync.process_one(unix_time);
            loop {
                match process {
                    full_optimistic::ProcessOne::Idle { sync: s } => {
                        sync = s;
                        break;
                    }
                    full_optimistic::ProcessOne::Finished {
                        sync: s,
                        finalized_blocks,
                    } => {
                        process = s.process_one(unix_time);

                        if let Some(last_finalized) = finalized_blocks.last() {
                            let mut lock = sync_state.lock().await;
                            lock.finalized_block_hash = last_finalized.header.hash();
                            lock.finalized_block_number = last_finalized.header.number;
                        }

                        for block in finalized_blocks {
                            for (key, value) in block.storage_top_trie_changes {
                                if let Some(value) = value {
                                    finalized_block_storage.insert(key, value);
                                } else {
                                    let _was_there = finalized_block_storage.remove(&key);
                                    // TODO: if a block inserts a new value, then removes it in the next block, the key will remain in `finalized_block_storage`; either solve this or document this
                                    // assert!(_was_there.is_some());
                                }
                            }
                        }
                    }

                    full_optimistic::ProcessOne::InProgress {
                        current_best_hash,
                        current_best_number,
                        resume,
                    } => {
                        // Processing has made a step forward.
                        // There is nothing to do, but this is used to update to best block
                        // shown on the informant.
                        let mut lock = sync_state.lock().await;
                        lock.best_block_hash = current_best_hash;
                        lock.best_block_number = current_best_number;
                        drop(lock);

                        process = resume.resume();
                    }

                    full_optimistic::ProcessOne::FinalizedStorageGet(req) => {
                        let value = finalized_block_storage
                            .get(&req.key_as_vec())
                            .map(|v| &v[..]);
                        process = req.inject_value(value);
                    }
                    full_optimistic::ProcessOne::FinalizedStorageNextKey(req) => {
                        // TODO: to_vec() :-/
                        let req_key = req.key().to_vec();
                        // TODO: to_vec() :-/
                        let next_key = finalized_block_storage
                            .range(req.key().to_vec()..)
                            .skip_while(move |(k, _)| &k[..] <= &req_key[..])
                            .next()
                            .map(|(k, _)| k);
                        process = req.inject_key(next_key);
                    }
                    full_optimistic::ProcessOne::FinalizedStoragePrefixKeys(req) => {
                        // TODO: to_vec() :-/
                        let prefix = req.prefix().to_vec();
                        // TODO: to_vec() :-/
                        let keys = finalized_block_storage
                            .range(req.prefix().to_vec()..)
                            .take_while(|(k, _)| k.starts_with(&prefix))
                            .map(|(k, _)| k);
                        process = req.inject_keys(keys);
                    }
                }
            }

            // Update the current best block, used for CLI-related purposes.
            {
                let mut lock = sync_state.lock().await;
                lock.best_block_hash = sync.best_block_hash();
                lock.best_block_number = sync.best_block_number();
            }

            // Start requests that need to be started.
            // Note that this is done after calling `process_one`, as the processing of pending
            // blocks can result in new requests but not the contrary.
            while let Some(action) = sync.next_request_action() {
                match action {
                    full_optimistic::RequestAction::Start {
                        start,
                        block_height,
                        source,
                        num_blocks,
                        ..
                    } => {
                        let (send_back, rx) = oneshot::channel();

                        let send_result = to_foreground
                            .send(FromBackground::RequestStart {
                                target: source.clone(),
                                request: network::protocol::BlocksRequestConfig {
                                    start: network::protocol::BlocksRequestConfigStart::Number(
                                        block_height,
                                    ),
                                    desired_count: num_blocks,
                                    direction: network::protocol::BlocksRequestDirection::Ascending,
                                    fields: network::protocol::BlocksRequestFields {
                                        header: true,
                                        body: true,
                                        justification: true,
                                    },
                                },
                                send_back,
                            })
                            .await;

                        // If the channel is closed, the sync service has been closed too.
                        if send_result.is_err() {
                            return;
                        }

                        let (rx, abort) = future::abortable(rx);
                        let request_id = start.start(abort);
                        block_requests_finished.push(rx.map(move |r| (request_id, r)));
                    }
                    full_optimistic::RequestAction::Cancel { user_data, .. } => {
                        user_data.abort();
                    }
                }
            }

            futures::select! {
                message = from_foreground.next() => {
                    let message = match message {
                        Some(m) => m,
                        None => return,
                    };

                    match message {
                        ToBackground::PeerConnected(peer_id) => {
                            let id = sync.add_source(peer_id.clone());
                            peers_source_id_map.insert(peer_id.clone(), id);
                        },
                        ToBackground::PeerDisconnected(peer_id) => {
                            let id = peers_source_id_map.remove(&peer_id).unwrap();
                            let (_, rq_list) = sync.remove_source(id);
                            for (_, rq) in rq_list {
                                rq.abort();
                            }
                        },
                    }
                },

                (request_id, result) = block_requests_finished.select_next_some() => {
                    // `result` is an error if the block request got cancelled by the sync state
                    // machine.
                    // TODO: clarify this piece of code
                    if let Ok(result) = result {
                        let result = result.map_err(|_| ()).and_then(|v| v);
                        let _ = sync.finish_request(request_id, result.map(|v| v.into_iter().map(|block| full_optimistic::RequestSuccessBlock {
                            scale_encoded_header: block.header.unwrap(), // TODO: don't unwrap
                            scale_encoded_extrinsics: block.body.unwrap(), // TODO: don't unwrap
                            scale_encoded_justification: block.justification,
                        })).map_err(|()| full_optimistic::RequestFail::BlocksUnavailable));
                    }
                },
            }
        }
    }
}
